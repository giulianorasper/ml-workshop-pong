{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-14T04:49:43.379969Z",
     "start_time": "2023-07-14T04:49:43.359676Z"
    }
   },
   "outputs": [],
   "source": [
    "import pygame\n",
    "\n",
    "import pong\n",
    "import printing\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "\n",
    "from pong import Pong\n",
    "from pong_observer import Player\n",
    "from main import train\n",
    "import strategy_pattern\n",
    "from typing import List\n",
    "from strategy_pattern import Strategy\n",
    "from play import play\n",
    "from pong_action import PongAction\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting to know the environment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Play a game of pong!\n",
    "W/S: move left paddle up/down\n",
    "UP/DOWN: move right paddle up/down"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training the neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What do you think, how long it will take until you can see a learning effect?\n",
    "Run the training and find out!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "strategy_pattern.strategies = {}\n",
    "train(training_time=60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next you will implement the relevant methods for a successful training yourself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the action space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Teach the neural network which actions exist by assigning a number to each action."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_action_map():\n",
    "    return {\n",
    "        0: PongAction(True, False), # move paddle up\n",
    "        # 1: ...\n",
    "        # 2: ...\n",
    "    }\n",
    "strategy_pattern.strategies[Strategy.ACTION_MAP] = get_action_map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the state space"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The input to a neural network are always numbers or a tuple of numbers; e.g. `(12, 3, 4, 5)`.\n",
    "We cant just throw a `Pong` object into the neural network and say \"learn from this\".\n",
    "Extract the relevant information from the `Pong` object and return it as a tuple of numbers.\n",
    "Note: Press `CTRL` and click on `Pong` in the code to go to the definition."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_state(observation: Pong) -> tuple:\n",
    "    return observation.ticks_this_ball_exchange, 42, 99 # ,observation.somethingElse,...\n",
    "\n",
    "strategy_pattern.strategies[Strategy.STATE] = get_state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test if you give the neural network the right information:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining the structure of the neural network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's not rely on the code to automatically create a neural network for us.\n",
    "What do you think, from what we did earlier, influences how the neural network must be structured?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remember, we use a list notation to define the structure of the neural network.\n",
    "E.g. E.g. `[4, 2, 3, 1]` created the following neural network:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"nn_structure.png\")\n",
    "display(im)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement you own network structure!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Enabled strategies: dict_keys([<Strategy.NETWORK_STRUCTURE: 4>])\n",
      "[INFO] using inferred n_observations\n",
      "[INFO] using inferred n_actions\n",
      "[INFO] using network structure: [1, 2, 2, 1]\n",
      "[INFO] Episode 1 / 0 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your network structure is not valid! It must match the state and action definitions!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 7\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m----> 7\u001B[0m     \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtraining_time\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/main.py:50\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(episodes, training_time)\u001B[0m\n\u001B[1;32m     49\u001B[0m pong\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m---> 50\u001B[0m \u001B[43mpong\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m dqn_agent\u001B[38;5;241m.\u001B[39mnext_episode()\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/pong.py:166\u001B[0m, in \u001B[0;36mPong.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnotify_observers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclock\u001B[38;5;241m.\u001B[39mtick(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mticks_per_second)  \u001B[38;5;66;03m# Limit the frame rate to 60 FPS\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/observer_pattern.py:15\u001B[0m, in \u001B[0;36mObservable.notify_observers\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m observer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservers:\n\u001B[0;32m---> 15\u001B[0m     \u001B[43mobserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/pong_observer.py:96\u001B[0m, in \u001B[0;36mPongObserver.update\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     95\u001B[0m     update: PongObservationUpdate \u001B[38;5;241m=\u001B[39m PongObservationUpdate(transition, next_state)\n\u001B[0;32m---> 96\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnotify_observers\u001B[49m\u001B[43m(\u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;66;03m# no we don't want to train\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/observer_pattern.py:15\u001B[0m, in \u001B[0;36mObservable.notify_observers\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m observer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservers:\n\u001B[0;32m---> 15\u001B[0m     \u001B[43mobserver\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/agent.py:214\u001B[0m, in \u001B[0;36mDQNAgent.update\u001B[0;34m(self, update)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transition \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 214\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransition\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m     state, action, next_state, reward \u001B[38;5;241m=\u001B[39m transition\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/agent.py:187\u001B[0m, in \u001B[0;36mDQNAgent.train\u001B[0;34m(self, transition)\u001B[0m\n\u001B[1;32m    186\u001B[0m next_state \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(next_state, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32, device\u001B[38;5;241m=\u001B[39mdevice)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m--> 187\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselect_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_train()\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/agent.py:163\u001B[0m, in \u001B[0;36mDQNAgent.select_action\u001B[0;34m(self, state, convert_to_tensor)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;66;03m# t.max(1) will return the largest column value of each row.\u001B[39;00m\n\u001B[1;32m    161\u001B[0m     \u001B[38;5;66;03m# second column on max result is index of where max element was\u001B[39;00m\n\u001B[1;32m    162\u001B[0m     \u001B[38;5;66;03m# found, so we pick action with the larger expected reward.\u001B[39;00m\n\u001B[0;32m--> 163\u001B[0m     q_values \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    164\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_action_value \u001B[38;5;241m=\u001B[39m q_values\u001B[38;5;241m.\u001B[39mmax(\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/agent.py:76\u001B[0m, in \u001B[0;36mDQN.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 76\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(x)\n",
      "File \u001B[0;32m~/PycharmProjects/pong-workshop/venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x6 and 1x2)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 9\u001B[0m\n\u001B[1;32m      7\u001B[0m     train(training_time\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m----> 9\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour network structure is not valid! It must match the state and action definitions!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mValueError\u001B[0m: Your network structure is not valid! It must match the state and action definitions!"
     ]
    }
   ],
   "source": [
    "def get_network_structure() -> List[int]:\n",
    "    # a neural network with 1 input, two hidden layers of size 2 and 1 output\n",
    "    return [1, 2, 2, 1]\n",
    "\n",
    "strategy_pattern.strategies[Strategy.NETWORK_STRUCTURE] = get_network_structure\n",
    "try:\n",
    "    train(training_time=1)\n",
    "except Exception as e:\n",
    "    raise ValueError(\"Your network structure is not valid! It must match the state and action definitions!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-14T04:52:43.361122Z",
     "start_time": "2023-07-14T04:52:42.705573Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see if it works!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining a reward function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will define a reward function i.e. tell the neural network what it did right and what it did wrong.\n",
    "But first, let's define a few helper functions:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def enemy_scored(observation: Pong, next_observation: Pong) -> bool:\n",
    "    \"\"\"\n",
    "    Did the enemy score a point?\n",
    "    :param observation: how the game state looks like\n",
    "    :param next_observation: how the game state looks like shortly after\n",
    "    :return: True if the enemy scored a point, False otherwise\n",
    "    :note: The enemy is the right side player!\n",
    "    \"\"\"\n",
    "    return True # replace with your own logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def how_far_is_ball_from_left_paddle(observation: Pong) -> float:\n",
    "    \"\"\"\n",
    "    How far is the ball from the paddle (horizontally)?\n",
    "    :param observation: how the game state looks like\n",
    "    :return: The horizontal distance between ball and the left paddle.\n",
    "    \"\"\"\n",
    "    return 0.0 # replace with your own logic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's define the reward function:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_reward(observation: Pong, next_observation: Pong) -> float:\n",
    "    \"\"\"\n",
    "    How good was the action the neural network took?\n",
    "    :param observation: how the game state looks like\n",
    "    :param next_observation: how the game state looks like shortly after\n",
    "    :return: A number indicating how good the action was (positive: good, negative: bad, 0: neutral).\n",
    "    \"\"\"\n",
    "    return 0 # your reward strategy goes here\n",
    "\n",
    "strategy_pattern.strategies = {}\n",
    "strategy_pattern.strategies[Strategy.REWARD] = get_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we actually train the neural network, test if the reward function works as expected.\n",
    "When running the following code, you can control the left paddle while observing the reward printed to the console:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Don't bother with the details of this code, but ask if you are interested\n",
    "import copy\n",
    "flags = copy.copy(printing.print_flags)\n",
    "printing.print_flags.append(printing.PrintFlag.REWARD)\n",
    "try:\n",
    "    play(invincible_enemy=True, debug=True)\n",
    "finally:\n",
    "    printing.print_flags = flags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see if it works!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transforming observations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have trained our agent on the left hand side, lets try it for the other side too!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(ai_enemy=True, swap_players=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What do you think? Great?!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Implement a function which \"transforms\" the observations,\n",
    "such that we can leverage what we already learned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def transform_observation(observation: Pong) -> Pong:\n",
    "    width = pong.width\n",
    "    height = pong.height\n",
    "\n",
    "strategy_pattern.strategies[Strategy.TRANSFORM_OBSERVATION] = transform_observation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's try again!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(ai_enemy=True, swap_players=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
