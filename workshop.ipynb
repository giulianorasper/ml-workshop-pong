{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pong\n",
    "import printing\n",
    "\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "\n",
    "from pong import Pong\n",
    "from pong_observer import Player\n",
    "from main import train\n",
    "import strategy_pattern\n",
    "from typing import List\n",
    "from strategy_pattern import Strategy\n",
    "from play import play\n",
    "\n",
    "RUNTIME=60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to train a pong agent. But before that play a couple of games to get a feel for the game.\n",
    "These are the controls:\n",
    "W/S: move left paddle up/down\n",
    "Up/Down (arrow keys): move right paddle up/down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train an agent. During training the agent controls the left paddle.\n",
    "The right paddle has a cheat code so that it always hits the ball."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(training_time=RUNTIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also play against the agent you just trained.\n",
    "You control the right paddle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(ai_enemy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the agent has learned something, but it is not very good.\n",
    "Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Implementing a strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you implement a function in this notebook, you override the default implementation. We call this implementing a new strategy.\n",
    "\n",
    "In the cell above, you see the implementation for a reward strategy, telling the agent if the action taken was \"good\" or \"bad\".\n",
    "Let's take a look at what happens when you override the default strategy for the reward function with something that is not very helpful for learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(observation: Pong, next_observation: Pong) -> float:\n",
    "    return 0 # no action is ever good nor bad\n",
    "\n",
    "strategy_pattern.strategies[Strategy.REWARD] = get_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the following cell, and then run training the agent should fail because\n",
    "a reward strategy is used that is not really helpful for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(training_time=RUNTIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though you will implement strategies one by one, the code will also work if you skip implementing a strategy.\n",
    "In this case the default strategy is used which means you can always test your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are stuck you can always reset the strategies to the default implementation by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_pattern.strategies = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify this by running the training again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(training_time=RUNTIME)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise: Implementing your own strategies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reward strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it is your turn to implement a reward strategy.\n",
    "\n",
    "The agent takes an action and then gives us two arguments:\n",
    "- `observation`: the observation of the game before the action has been taken.\n",
    "- `next_observation`: the observation of the game after the action has been taken.\n",
    "\n",
    "Then the agent asks: \"Was the action I took good or bad? Did something change that can be considered good or bad?\".\n",
    "\n",
    "That answer is for you to decide:\n",
    "- higher positive rewards are good\n",
    "- lower negative rewards are bad\n",
    "- zero rewards are neutral\n",
    "(there are other ways for classifying good, bad and neutral, but let's keep it simple for now)\n",
    "\n",
    "\n",
    "Note:\n",
    "Both arguments are full copies of the `Pong` class. You can take a look at the `Pong` class by shift-clicking on the class name below.\n",
    "Then you can decide which information may be relevant for deciding a reward.\n",
    "\n",
    "Als keep in mind that we train an agent controlling the left paddle. Choose the rewards accordingly!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_reward(observation: Pong, next_observation: Pong) -> float:\n",
    "    return 0 # your reward strategy goes here\n",
    "\n",
    "strategy_pattern.strategies[Strategy.REWARD] = get_reward"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we blindly run the training again you should check, that your reward strategy does what you intended.\n",
    "Run the following cell to control the left paddle. The console now prints the reward for each action (neutral rewards are omitted)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Don't bother with the details of this code, but ask if you are interested\n",
    "import copy\n",
    "flags = copy.copy(printing.print_flags)\n",
    "printing.print_flags.append(printing.PrintFlag.REWARD)\n",
    "try:\n",
    "    play(invincible_enemy=True, debug=True)\n",
    "finally:\n",
    "    printing.print_flags = flags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Everything looks good? Then let's train the agent again:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=RUNTIME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## State strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The thing which is the input to the neural network is called the state.\n",
    "Neither do we need, nor want, nor can input the whole pong into the neural network. A neural network only takes numbers as input!\n",
    "So we need to decide which information is relevant for the agent to make a decision.\n",
    "\n",
    "The state strategy is a function that takes the `Pong` class and should return a tuple of relevant information (called features)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(observation: Pong) -> str:\n",
    "    return observation.ticks_this_ball_exchange, 42, 99 # ,observation.somethingElse,... your state strategy goes here\n",
    "\n",
    "strategy_pattern.strategies[Strategy.STATE] = get_state"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Train the agent again:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=RUNTIME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Action strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The neural network can predict a value for each possible action. In our algorithm the action with the highest value is chosen.\n",
    "For the neural network, an action is just the index of the associated neuron in the output/last layer.\n",
    "\n",
    "Implement an action strategy by assigning an action to take in case the associated output neuron has the highest value.\n",
    "\n",
    "Hint:\n",
    "The first boolean in the `PongAction` constructor is for moving the paddle up, the second for moving it down. It corresponds to the associated keys being pressed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pong_action import PongAction\n",
    "\n",
    "\n",
    "def get_action_map():\n",
    "    return {\n",
    "        0: PongAction(False, False), # do nothing\n",
    "        # 1: ...\n",
    "        # 2: ...\n",
    "    }\n",
    "strategy_pattern.strategies[Strategy.ACTION_MAP] = get_action_map"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Network structure strategy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following code the returned list represents the structure of the neural network.\n",
    "\n",
    "E.g. `[4, 2, 3, 1]` created the following neural network:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "im = Image.open(\"nn_structure.png\")\n",
    "display(im)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before we deal with the question of \"Which network structure is best?\" let's first run the following code which defines and immediately runs training for only 1 second:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_network_structure() -> List[int]:\n",
    "    # a neural network with 1 input, two hidden layers of size 2 and 1 output\n",
    "    return [1, 2, 2, 1]\n",
    "\n",
    "strategy_pattern.strategies[Strategy.NETWORK_STRUCTURE] = get_network_structure\n",
    "train(training_time=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Huh? An error? What happened? Figure it out and fix it!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have a working network structure, let's try to find a better one.\n",
    "Try out different network structures and see how they perform.\n",
    "\n",
    "Note:\n",
    "You invest a lot of time in randomly trying out different network structures.\n",
    "It is enough to try a few out and settle with one that works just \"okay\"."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_network_structure() -> List[int]:\n",
    "    # a neural network with 1 input, two hidden layers of size 2 and 1 output\n",
    "    return [1, 2, 2, 1]\n",
    "\n",
    "strategy_pattern.strategies[Strategy.NETWORK_STRUCTURE] = get_network_structure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=RUNTIME)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pushing the limits"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets train the agent for longer!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(training_time=60*5) # 5 minutes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, lets play against the agent again!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(ai_enemy=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Transforming observations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have trained our agent on the left hand side, lets try it for the other side too!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(ai_enemy=True, swap_players=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It doesn't work! Why?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay then let's just make the agent thing that it is playing on the left hand side.\n",
    "Implement a function which transforms the observation in such a way, that the agent can leverage what it has learned on the left hand side!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_observation(observation: Pong) -> Pong:\n",
    "    width = pong.width\n",
    "    height = pong.height\n",
    "\n",
    "strategy_pattern.strategies[Strategy.TRANSFORM_OBSERVATION] = transform_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's test if it works:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(ai_enemy=True, swap_players=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
